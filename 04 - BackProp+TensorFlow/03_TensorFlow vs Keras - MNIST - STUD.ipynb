{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal artificial en Numpy - MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografía:\n",
    "- Deep Learning with Python (capítulo 2), Francois Chollet, 2018 Manning\n",
    "- https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional, Andrew Ng, 2018 DeepLearning.ai\n",
    "- Learning Tensor Flow, Tom Hope, Yehezkel S. Resheff & Itay Lieder, 2017 O'Reilly\n",
    "- TensorFlow for Deep Learning, Bharath Ramsundar & Reza Bosagh Zadeh, 2018, O'Reilly\n",
    "- Python Machine Learning, 2nd ed. (capítulo 13), Sebastian Rachska, 2017 Packt\n",
    "- Machine Learning with TensrFlow (capítulo 7), Nishant Shukla, 2018 Manning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos en este notebook a implementar una red neuronal que permita la clasificación de imágenes de dígitos en escala de grises, utilizando tres niveles de extracción de programación.\n",
    "\n",
    "Además de utilizar **numpy**, aprovechamos para presentar las librerías **tensorflow** y **keras** a través de su aplicación al problema de clasificación mencionado. \n",
    "El modelo a implementar es una red con una capa escondida de 512 neuronas utilizando la función de activación **ReLU** y 10 neuronas de salida utilizando la función de activación **softmax**.\n",
    "\n",
    "Vamos a seguir un protocolo de *holdout* para evaluar los clasificadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1. Exploración y entendimiento del dataset MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras necesita una base para trabajar. Vamos a usar tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras viene con MNIST directamente integrada. Las imagenes y sus etiquetas (clases) ya vienen particionadas en training y test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images_ori, train_labels_ori), (test_images_ori, test_labels_ori) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_ori[0][5] #La primera imagen, la 6a fila, muestra los 28 valores de gris (uno por cada columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_ori.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera dimensión del tensor de train_images es de 60000, pues hay 60000 imagenes, las 2 siguientes dimensiones son las que dan el tamaño de las imágenes (28x28). Como son imagenes en escala de grises, no hay una 4a dimensión que tendría el canal de color (e.g. RGB). Cada valor de la matriz de 28x28 es una valor de gris de 0 a 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (train_labels_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (test_labels_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El testset tiene 10000 imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_ori.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver cada una de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = train_images_ori[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos que los datos de entrenamiento y test estén en el formato dado por la capa de entrada de la red, donde se reciben los datos representados a partir de un tensor cuya primera dimensión tiene 784 neuronas, es decir, recibe como input instancias representadas por tensores con dimensionalidad (784,).\n",
    "\n",
    "Cómo los datos de entrenamiento están en tensores de 60000x28x28, es necesario convertirlos en un tensor de 60000x784. Idem para el tensor de test.\n",
    "\n",
    "Vamos también a modificar la escala de grises, que originalmente va de 0 a 255, a una escala que vaya de 0 a 1. Para esto, dividimos los valores originales por 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la estructura de la red (número de neuronas por capa y número de inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 28*28\n",
    "n1 = 512\n",
    "n2 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplanar los datos de train y test para poder utilizar cada pixel como un predictor (cada pixel en una columna diferente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images_ori\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images_ori\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas están en valores numéricos, vamos codificarlos en one hot encoding, con un vector de K posiciones para K clases, donde la clase específica de cada ejemplo tiene un valor de 1, y las demas posiciones del vector tienen valores de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_ori[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels_ori)\n",
    "test_labels = to_categorical(test_labels_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los labels están desordenados y no responde a ninguna organización sistemática, los podemos dejar así. Si siguieran un orden definido, sería necesario barajarlos para que el orden no influyera en el aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_images.T\n",
    "y_train = train_labels.T\n",
    "m=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2. MNIST: ANN con TensorFlow de bajo nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del dataflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante aclarar que la tarea inicial de definir el dataflow graph no implica ningún procesamiento numérico, solo el establecimiento del orden de las operaciones y de cómo se conectan entre ellas (lazy evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hacemos es indicar las características de los inputs y outputs y crear los **placeholders** correspondientes: son una especie de variables a las cuales se les asignarán valores en un momento futuro, el medio a través del cual se envían datos a los grafos de TensorFlow:\n",
    "- los inputs son las imágenes, cada una caracterizada por las 784 columnas correspondientes a los pixeles.\n",
    "- los outputs son los 10 indicadores de las variables de salida del modelo multinomial (one hot encoding).\n",
    "\n",
    "Tenemos que los *shapes* tienen algunos de los rangos cuyos números de dimensiones no están determinados prealablemente, para estos especificamos entonces el valor de **None**. Este es el caso del rango dedicado al número de registros de entrenamiento que vamos a utilizar (o de predicción), que no se puede ni debe definir estáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, n0], dtype=tf.float32, name=\"input_X\")  \n",
    "y = tf.placeholder(shape=[None, n2], dtype=tf.float32, name=\"labels_Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros de la red (pesos y sesgos) se definen en otra estructura llamada **Variable**, diferente a los **placeholders**. Las variables son manipuladas durante los cálculos, sus valores son modificados durante el entrenamiento.\n",
    "\n",
    "Los pesos de las conexiones entre capas se inicializan aleatoriamente; los sesgos se pueden inicializar en zeros.\n",
    "No olvidemos inicializar el generador aleatorio de números para poder reproducir los resultados\n",
    "\n",
    "Vamos a utilizar el generador pseudo aleatorio de TensorFlow, por lo que debemos inicializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1234\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random_normal((n0, n1), stddev=0.1), name=\"capa1_W\")\n",
    "w2 = tf.Variable(tf.random_normal((n1, n2), stddev=0.1), name=\"capa2_W\")\n",
    "b1 = tf.Variable(tf.zeros((n1)), dtype=tf.float32, name=\"capa1_b\")\n",
    "b2 = tf.Variable(tf.zeros((n2)), dtype=tf.float32, name=\"capa2_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones de entrenamiento y predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos definir las operaciones de la etapa de **feed-forward**, calculando directamente los valores de las activaciones de las capas 1 y 2. La capa 1 va a tener la tangente hiperbólica como función de activación, y la capa 2.\n",
    "\n",
    "Es importante aclarar que TensorFlow incluye la activación **SoftMax** directamente con la función de costo de **cross-entropy**, por lo que no es necesario especificar la operación en la capa 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = X\n",
    "#a1 = tf.nn.relu(tf.matmul(a0, w1)+b1, name=\"capa1_activacion\")\n",
    "a1 = tf.nn.tanh(tf.matmul(a0, w1)+b1, name=\"capa1_activacion\")\n",
    "a2 = tf.matmul(a1, w2, name=\"capa2_activacion\")+b2\n",
    "y_pred = a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La etapa del **back-propagation** consiste en definir la función de costo que se utiliza, en este caso cross-entropy, y el optimizador para minimizarla.\n",
    "Vamos a minimizar la función de costo, que definimos como un nodo del grafo ya que es una operación entre tensores, utilizando el método de descenso de gradiente (un paso de GD es otro nodo en el grafo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5 #learning rate\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred), name=\"costo\")\n",
    "paso_gd = tf.train.GradientDescentOptimizer(lr, name=\"GD_Step\").minimize(cost, name=\"min_GD_Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos otra tarea que sirve para predecir la categoría de un elemento dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = tf.argmax(a2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución del dataflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la sesión de tensor flow donde vamos a computar el dataflow graph e inicializar las variables pre definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos el dataflow graph utilizando un servicio web de TensorBoard (código copiado de: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora corremos las iteraciones de aprendizaje de los parámetros de la red neuronal; cada época está definida en por un paso del gradient descent que definimos. Especificamos entonces los valores de los placeholders de **X y Y**.\n",
    "Después de cada época evaluamos como nos está yendo en el training set y en el test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los labels están desordenados y no responde a ninguna organización sistemática, los podemos dejar así. Si siguieran un orden definido, sería necesario barajarlos para que el orden no influyera en el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los hiperparámetros del modelo, utilizados para aprender los pesos de las capas (los verdaderos parámetros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 100 # Iteraciones de backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_images.T\n",
    "y_train = train_labels.T\n",
    "m=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_images.T\n",
    "y_test = test_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 100 # Iteraciones de backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoca in range(epocas):\n",
    "    sess.run(paso_gd, feed_dict={X: X_train.T, y: y_train.T})\n",
    "    y_train_pred = sess.run(predict, feed_dict={X: X_train.T, y: y_train.T})\n",
    "    y_test_pred = sess.run(predict, feed_dict={X: X_test.T, y: y_test.T})\n",
    "    \n",
    "    y_train_real = np.argmax(y_train, axis=0)\n",
    "    train_accuracy = np.mean(np.equal(y_train_real, y_train_pred))\n",
    "    y_test_real = np.argmax(y_test, axis=0)\n",
    "    test_accuracy = np.mean(np.equal(y_test_real, y_test_pred))\n",
    "    test_accuracy\n",
    "    \n",
    "    if epoca%10 ==0:\n",
    "        print(\"Epoca: %d, train accuracy = %.4f%%, test accuracy = %.4f%%\"\n",
    "              % (epoca, 100. * train_accuracy, 100. * test_accuracy))\n",
    "\n",
    "print(\"Epoca: %d, train accuracy = %.4f%%, test accuracy = %.4f%%\"\n",
    "    % (epoca, 100. * train_accuracy, 100. * test_accuracy))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_pred, y_test_real))\n",
    "print(classification_report(y_test_pred, y_test_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3. Modelo utilizando el módulo *layers* de TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a utilizar el módulo **layers**, que permite abstraer las capas de tal manera que no es necesario definici sus operaciones internas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a limpiar el grafo por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "RANDOM_SEED = 1234\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los placeholders. Vamos a ilustrar como se puede utilizar le método one_hot de TF para transformar de un array de valores con las categorías a uno codificado one hot dentro de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=(None, n0), dtype=tf.float32, name=\"Inputs_imagenes\")  \n",
    "y_una_cifra = tf.placeholder(shape=(None), dtype=tf.uint8, name=\"Labels\")\n",
    "y_onehot = tf.one_hot(indices=y_una_cifra, depth=n2, name=\"Labels_1hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las capas de la red, con sus funciones de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = tf.layers.dense(inputs=X, units=n1, activation=tf.nn.tanh, name=\"Capa_escondida\")\n",
    "#w1 = tf.Variable(tf.random_normal((n0, n1), stddev=0.1))\n",
    "#b1 = tf.Variable(tf.zeros((n1)), dtype=tf.float32)\n",
    "logits = tf.layers.dense(inputs=h1, units=n2, activation=None, name=\"Logits_pred\")\n",
    "#w2 = tf.Variable(tf.random_normal((n1, n2), stddev=0.1))\n",
    "#b2 = tf.Variable(tf.zeros((n2)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las operaciones de prediccion, tanto de clases como de probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {\n",
    "    'clases': tf.argmax(logits, axis=1, name=\"pred_clases\"),\n",
    "    'probas': tf.nn.softmax(logits, name=\"softmax\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones de incialización, costo, optimizador y paso de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "costo = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, logits=logits)\n",
    "optimizador = tf.train.GradientDescentOptimizer(learning_rate=lr, name=\"GD_optim\")\n",
    "pasoGD = optimizador.minimize(loss = costo, name=\"pasoGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el ciclo de entrenamiento de backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess =tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoca in range(epocas):\n",
    "    costos = []\n",
    "    feed = {X: train_images, y_una_cifra:train_labels_ori}\n",
    "    _, costo_epoca, y_train_pred_class, y_train_pred_proba = sess.run(\n",
    "        [pasoGD, costo, preds['clases'], preds['probas']], feed_dict=feed)\n",
    "    train_accuracy = np.mean(np.equal(train_labels_ori, y_train_pred_class))\n",
    "\n",
    "    feed = {X: test_images, y_una_cifra:test_labels_ori}\n",
    "    y_test_pred_class, y_test_pred_proba = sess.run(\n",
    "        [preds['clases'], preds['probas']], feed_dict=feed)\n",
    "    test_accuracy = np.mean(np.equal(test_labels_ori, y_test_pred_class))\n",
    "\n",
    "    if epoca%10 ==0:\n",
    "        costos.append(costo_epoca)\n",
    "        print(\"Epoca %2d: costo: %.6f, acc_train=%.6f, acc_test=%.6f\" \n",
    "              % (epoca, np.mean(costos), train_accuracy, test_accuracy))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_pred_class, test_labels_ori))\n",
    "print(classification_report(y_test_pred_class, test_labels_ori))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por alguna razón, TensorFlow no cuenta con una implementación de mini-batch, pero nada nos impide implementarla (no lo haremos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4. MNIST: ANN con Keras Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras es una capa de abstracción por encima de un framework de Deep Learning de backend.\n",
    "Los backends actualmente soportados por Keras son: TensorFlow (Google), Theano (Université de Montréal), CNTK (Microsoft).\n",
    "Gracias a este nivel de abstracción, se podría escribir un solo código y cambiar por completo de backend sin necesidad de modificar una línea de código.\n",
    "Keras ha sido adoptado como parte de TensorFlow por Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras tiene dos tipos de modelo: \n",
    "- Secuencial: diseñado para arquitecturas de red senciallas, limitándos a procesamientos encadenados en un solo sentido\n",
    "- Funcional: flexible, permite cualquier tipo de estructura general de procesamiento con o sin ciclos, con una o varias salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módelo secuencial de Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos entonces el mismo modelo de una capa escondida que con los otros dos códigos:\n",
    "- la capa de entrada no se especifica directamente, esta va a tener 28x28 = 784 nodos de entrada, 1 por cada pixel de las imágenes\n",
    "- una capa escondida de 512 neuronas, que utiliza una función de activación **RELU**\n",
    "- una capa de salida de 10 neuronas, una para cada clase, que utiliza una función de activación **softmax**, que permite obtener en la salida una distribución de probabilidad por cada una de las clases posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(n1, activation='tanh', input_shape=(n0,)))\n",
    "network.add(layers.Dense(n2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que las capas son más profundas, las representaciones de los imputs van cambiando.\n",
    "Cada capa se puede interpretar como un filtro, los datos entran con una representación y salen con una representación más útil para la tarea en cuestión (en este caso, la clasificación de imágenes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que el modelo quede listo para su uso, hay que compilarlo y definir 3 cosas:\n",
    "* una **función de pérdida**: que define como se va medir la calidad del modelo al aplicarlo a los datos de entrenamiento, entre más baja la pérdida, mejor la calidad de la tarea ejecutada.\n",
    "* un **optimizador**: el mecanismo que utiliza la red para actualizar los pesos, basándose en la función de perdida.\n",
    "* las **métricas**: a monitorear durante el entrenamiento y evaluación de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de perdida es la **categorical_crossentropy**, que se utiliza en el caso de problemas de clasificación multiclase. El optimizador tratará de minimizarla. Es una función de distancia entre el valor real (que solo tiene una de las dimensiones con un 1 y las demás con un 0) y el predicho por el modelo (que tiene una distribución que probablemente no sea absoluta, sino que reparta la unidad de probabilidad en cada una de las clases).\n",
    "Esta función es la suma negativa de las entropias de cada una de las posiciones del vector one hot encoded de clases:\n",
    "$$H(y,\\tilde{y}) =-\\sum{y*log(\\tilde{y})}$$\n",
    "Como el valor de *y* es solo 1 en una de las posiciondes del vector de clases y 0 en las demás, solo una posición será considerada, de tal manera que la similitud entre los valores reales (1) y predichos (fracción de 1) es dada por el log del valor predicho para la clase real.  \n",
    "\n",
    "Vamos a utilizar un optimizador **SGD** (Stochastic Gradient Descent), pero sin sacarle provecho a su funcionalidad de modificaicón del learning rate, de tal manera que quede igual al gradient descent más simple.\n",
    "\n",
    "Para hacerle seguimiento a la evolución de la calidad del modelo, utilizaremos la métrica de **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epocas = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=lr, decay=0, momentum=0, nesterov=False)\n",
    "network.compile(optimizer=sgd,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se tiene que aprender el modelo, llamando al método *fit* (lo que es diferente a compilarlo). Se tiene entonces que:\n",
    "* definir los datos de entrada (train_images) y sus clases correspondientes (train_labels), cuyas dimensiones corresponden a las establecidas por la red que definimos.\n",
    "* establecer el número de épocas o generaciones de entrenamiento, i.e. el número de veces que se va a pasar el dataset de entrenamiento por el modelo.\n",
    "* establecer el tamaño de los paquetes de entrada a considerar antes de hacer cada actualización. Se podría definir por ejemplo en 128 imágenes. Es decir que para cada época que considera 60000 imágenes de entrenamiento, se tendrían 469 batch de entrenamiento de 128 imágenes cada uno, y cada batch se actualizarían los parámetros de la red, sin necesidad de acabar una época. Vamos a establecer un tamaño de batch de 60000, para que solo se actualicen los parámetros una vez se procesen todas las imágenes, y poder así comparar con los resultados de los modelos anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=epocas, batch_size=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtuvo al final 91.15% de exactitud con el set de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos definir un batch size más pequeño, como lo hicimos con la red que creamos con numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=epocas, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluemos ahora con el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtuvo 98.18% de exactitud en el set de datos de evaluación, inferior al 98.88% del set de entrenamiento. Siempre hay que valuar con un dataset diferente al de entrenamiento para evitar sobreestimar la calidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede mejorar utilizando métodos de regularización como las normas L1 o L2. En TF, las capas regularizadas incluyen el método en cuestión. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "network.add(layers.Dense(n1, activation='tanh', input_shape=(n0,), kernel_regularizer=regularizers.l2(0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5. Módelo funcional de Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gran diferencia entre el API **Functional** de Keras con las del **Sequential**, es la flexibilidad:\n",
    "- se pueden definir múltiples tensores de entrada y de salida\n",
    "- se pueden tener capas utilizadas por varios modelos\n",
    "- se pueden definir ciclos en las conexiones de las capas\n",
    "- se pueden reutilizar modelos funcionales como si fueran capas para definir otros modelos que los utilizan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Flatten, Dense#, Lambda\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un tensor **Input** de entrada que define las dimensiones de entrada, que internamente va  crear un **placeholder** en TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape = (784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora las capas densas, especificando el número de neuronas a utilizar y las funciones de activación correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_output = Dense(n1, activation = 'tanh')(input_tensor)\n",
    "classification_output = Dense(n2, activation = 'softmax')(hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos todas las capas conectadas. Vamos ahora a crear el modelo que vamos a entrenar, definiendo el tensor donde quedarán los datos de entrada y el tensor donde quedarán los datos de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_tensor], [classification_output])\n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a entrenar el modelo, para lo que hay que definir el optimizador que vamos a utilizar, la función de pérdida que se va a minimizar y la métrica que se quiere monitorear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(weights)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"test loss: {}, test accuracy: {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con 1 época, el training set se llega a 91.8%, mientras que el test set produce 95.5% de accuracy. Vamos a intentar con un batch size de 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1234)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.set_weights(weights)\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=50, validation_split=0.2)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"test loss: {}, test accuracy: {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con 10 épocas llegamos a 96.6% en el training set y 98% en el test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto **history** retornado por el entrenamiento (**fit**) permite tener una idea más clara de como se realizó el entrenamiento. Vamos a plotear la información que contiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar los accuracy del training set y del test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y lo propio con el loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
